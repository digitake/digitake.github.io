---
layout: post
title:  "ML Terminology"
date:   2017-06-27 03:30:00 +0700
categories: ML, DL, Terminology
comments: yes
---

- (Batch) Gradient Descent: Well known method in Machine learning to 
learn minimize the cost function.
 
- Stochastic Gradient Descent(SGD): A gradient descent thats start tuning
 parameters by using a single training data.
 
- Minibatch Gradient Descent: A gredient descent thats tunes parameters 
based on subset of training data. Oftenly, it performs better than SGD and
train faster than Batch Gradient Descent.

- Momentum(of Gradient Descent): An average value of SGD or minibatch.

- Decay: A parameter to reduce learning rate when the traning converge to the
optiomal point.

